{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read the file with encoding: utf-8\n",
      "File successfully read using encoding: latin1\n"
     ]
    }
   ],
   "source": [
    "# Membaca data dari file CSV\n",
    "# Try reading the file with different encodings\n",
    "encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv('Daftar_Skripsi.csv', encoding=encoding)\n",
    "        print(f\"File successfully read using encoding: {encoding}\")\n",
    "        # Further processing or analysis on the dataframe\n",
    "        break  # Stop the loop if the file is read successfully\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Could not read the file with encoding: {encoding}\")\n",
    "\n",
    "# If none of the encodings work, consider further inspection or conversion of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membersihkan simbol dan angka dari teks\n",
    "def remove_symbols_and_numbers(text):\n",
    "    # Menggunakan ekspresi reguler untuk menghilangkan simbol dan angka\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "    \n",
    "# Terapkan remove_symbols_and_numbers pada kolom 'JudulSkripsi' dan membuat kolom baru judul_cleaned\n",
    "df['judul_cleaned'] = df['JudulSkripsi'].apply(remove_symbols_and_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisasi dan Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi dan case folding pada kolom judul\n",
    "df['judul_tokenized'] = df['judul_cleaned'].apply(lambda x: word_tokenize(str(x)))\n",
    "df['judul_lower'] = df['judul_tokenized'].apply(lambda x: [word.lower() for word in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword from file\n",
    "list_stopwords = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def stopwords_removal(words, stopwords):\n",
    "    return [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "# Read stopwords from the file stopwords.txt\n",
    "with open('stopwords.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# Assuming 'data' contains the DataFrame and 'judul_lower' column is the list of words\n",
    "# Apply stopwords_removal to each list in 'judul_lower' column\n",
    "df['judul_no_stopwords'] = df['judul_lower'].apply(lambda x: stopwords_removal(x, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\frans\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\frans\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Function to perform stemming\n",
    "def apply_stemming(words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)  # Join stemmed words back into a sentence\n",
    "\n",
    "# Apply stemming to each list in 'judul_lower' column\n",
    "df['judul_stemmed'] = df['judul_no_stopwords'].apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kumpulan kata dasar = df['judul_stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' contains the DataFrame and 'judul_stemmed' column has the stemmed text\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the stemmed text data using TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['judul_stemmed'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for further analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.02473705 0.        ]\n",
      " [0.         0.         0.02122812 ... 0.         0.         0.02538005]\n",
      " [0.0164457  0.01333895 0.00978795 ... 0.01279429 0.         0.        ]]\n",
      "\n",
      "Cluster Counts:\n",
      "4    27\n",
      "2    14\n",
      "3    12\n",
      "1     8\n",
      "0     5\n",
      "Name: cluster_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans clustering\n",
    "num_clusters = 5  # Ubah jumlah kluster sesuai kebutuhan\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans to the TF-IDF matrix\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Predict cluster labels for the data points\n",
    "df['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# Optional: Print the cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "print(kmeans.cluster_centers_)  # Centroids of clusters\n",
    "\n",
    "# Optional: Analyze the cluster assignments\n",
    "cluster_counts = df['cluster_label'].value_counts()\n",
    "print(\"\\nCluster Counts:\")\n",
    "print(cluster_counts)  # Number of data points in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         JudulSkripsi  cluster_label\n",
      "0   Alat Pendeteksi Kebocoran Gas Lpg Berbasis Ard...              1\n",
      "1   Sistem Pendukung Keputusan Penilaian Kinerja P...              4\n",
      "2   Membangun Pengenalan Hewan Augmented Reality B...              2\n",
      "3   Alat Penyiram Tanaman Otomatis Menggunakan Ard...              1\n",
      "4   Desain Dan Implementasi E-Learning Pada Lkp Ac...              5\n",
      "..                                                ...            ...\n",
      "61  Aplikasi Pengolahan Data Sparepart Pada Pt Kal...              5\n",
      "62  Aplikasi Monitoring Bukti Potong Pph Pasal 15 ...              5\n",
      "63       Aplikasi Kebugaran Tubuh Berbasis Multimedia              5\n",
      "64  Sistem Informasi Data Kepegawaian Pada Kantor ...              5\n",
      "65  Aplikasi Pengolahan Data Service Handphone Pad...              5\n",
      "\n",
      "[66 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Misalkan 'df' adalah DataFrame yang Anda miliki\n",
    "df['cluster_label'] = df['cluster_label'] + 1\n",
    "\n",
    "# Print DataFrame setelah menambahkan 1 pada setiap nilai di kolom 'cluster_label'\n",
    "print(df[['JudulSkripsi', 'cluster_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         JudulSkripsi  cluster_label\n",
      "0   Alat Pendeteksi Kebocoran Gas Lpg Berbasis Ard...              1\n",
      "43  Penerapan Spk Decission Tree Menggunakan Algor...              1\n",
      "37  Sistem Pendeteksi Peringatan Dini Keamanan Rum...              1\n",
      "23  Game Edukasi Drag And Drop Budaya Nusantara Un...              1\n",
      "3   Alat Penyiram Tanaman Otomatis Menggunakan Ard...              1\n",
      "..                                                ...            ...\n",
      "34  Sistem Informasi Pengelolaan Media Pembelajara...              5\n",
      "40  Aplikasi Pemilihan Handphone Pada Parindo Cell...              5\n",
      "41  Implementasi Absensi Siswa Berbasis Sms Gatewa...              5\n",
      "45  Pengembangan Sistem Pengolahan Data Gaji Karya...              5\n",
      "65  Aplikasi Pengolahan Data Service Handphone Pad...              5\n",
      "\n",
      "[66 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Misalkan 'df' adalah DataFrame yang Anda miliki\n",
    "sorted_df = df[['JudulSkripsi', 'cluster_label']].sort_values('cluster_label')\n",
    "\n",
    "# Print DataFrame yang sudah diurutkan berdasarkan kolom 'cluster_label'\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     int64\n",
      "JudulSkripsi          object\n",
      "NamaPeneliti          object\n",
      "Tahun                  int64\n",
      "ProgramStudi          object\n",
      "judul_cleaned         object\n",
      "judul_tokenized       object\n",
      "judul_lower           object\n",
      "judul_no_stopwords    object\n",
      "judul_stemmed         object\n",
      "cluster_label          int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
