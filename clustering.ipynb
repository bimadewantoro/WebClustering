{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read the file with encoding: utf-8\n",
      "File successfully read using encoding: latin1\n"
     ]
    }
   ],
   "source": [
    "# Membaca data dari file CSV\n",
    "# Try reading the file with different encodings\n",
    "encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv('Daftar_Skripsi.csv', encoding=encoding)\n",
    "        print(f\"File successfully read using encoding: {encoding}\")\n",
    "        # Further processing or analysis on the dataframe\n",
    "        break  # Stop the loop if the file is read successfully\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Could not read the file with encoding: {encoding}\")\n",
    "\n",
    "# If none of the encodings work, consider further inspection or conversion of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membersihkan simbol dan angka dari teks\n",
    "def remove_symbols_and_numbers(text):\n",
    "    # Menggunakan ekspresi reguler untuk menghilangkan simbol dan angka\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "    \n",
    "# Terapkan remove_symbols_and_numbers pada kolom 'JudulSkripsi' dan membuat kolom baru judul_cleaned\n",
    "df['judul_cleaned'] = df['JudulSkripsi'].apply(remove_symbols_and_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisasi dan Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi dan case folding pada kolom judul\n",
    "df['judul_tokenized'] = df['judul_cleaned'].apply(lambda x: word_tokenize(str(x)))\n",
    "df['judul_lower'] = df['judul_tokenized'].apply(lambda x: [word.lower() for word in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword from file\n",
    "list_stopwords = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def stopwords_removal(words, stopwords):\n",
    "    return [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "# Read stopwords from the file stopwords.txt\n",
    "with open('stopwords.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# Assuming 'data' contains the DataFrame and 'judul_lower' column is the list of words\n",
    "# Apply stopwords_removal to each list in 'judul_lower' column\n",
    "df['judul_no_stopwords'] = df['judul_lower'].apply(lambda x: stopwords_removal(x, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\frans\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\frans\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Function to perform stemming\n",
    "def apply_stemming(words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)  # Join stemmed words back into a sentence\n",
    "\n",
    "# Apply stemming to each list in 'judul_lower' column\n",
    "df['judul_stemmed'] = df['judul_no_stopwords'].apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kumpulan kata dasar = df['judul_stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' contains the DataFrame and 'judul_stemmed' column has the stemmed text\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the stemmed text data using TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['judul_stemmed'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for further analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "    # Melakukan clustering menggunakan KMeans\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "df[\"cluster_label\"] = kmeans.labels_\n",
    "\n",
    "# Mendapatkan informasi tentang cluster\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_counts = df[\"cluster_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        judul_stemmed  cluster_label  \\\n",
      "0   alat deteksi bocor gas lpg bas arduino uno sms...              0   \n",
      "1   sistem dukung putus nilai kerja pegawai honore...              3   \n",
      "2   bangun kenal hewan augmented reality bas andro...              1   \n",
      "3   alat siram tanam otomatis arduino uno kendali sms              0   \n",
      "4   desain implementasi elearning lkp active engli...              4   \n",
      "..                                                ...            ...   \n",
      "61  aplikasi olah data sparepart pt kaltim prima u...              4   \n",
      "62  aplikasi monitoring bukti potong pph pasal pt ...              4   \n",
      "63                aplikasi bugar tubuh bas multimedia              4   \n",
      "64  sistem informasi data pegawai kantor desa loa ...              4   \n",
      "65  aplikasi olah data service handphone orange ph...              4   \n",
      "\n",
      "    average_distance_within_cluster  \n",
      "0                          0.694108  \n",
      "1                          0.816303  \n",
      "2                          0.586690  \n",
      "3                          0.711998  \n",
      "4                          0.933261  \n",
      "..                              ...  \n",
      "61                         0.907385  \n",
      "62                         0.936129  \n",
      "63                         0.905203  \n",
      "64                         0.915209  \n",
      "65                         0.887012  \n",
      "\n",
      "[66 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Get the indices of documents in each cluster\n",
    "cluster_indices = [df.index[df['cluster_label'] == cluster_num].tolist() for cluster_num in range(num_clusters)]\n",
    "\n",
    "# Function to calculate average distance within a cluster for a given document index\n",
    "def average_distance_within_cluster(doc_index, cluster_indices):\n",
    "    cluster_num = df.loc[doc_index, 'cluster_label']\n",
    "    cluster_docs = cluster_indices[cluster_num]\n",
    "\n",
    "    # Calculate pairwise distances between the given document and all other documents in the same cluster\n",
    "    distances = pairwise_distances(tfidf_matrix[doc_index], tfidf_matrix[cluster_docs], metric='cosine')[0]\n",
    "\n",
    "    # Calculate average distance\n",
    "    average_distance = sum(distances) / len(distances)\n",
    "\n",
    "    return average_distance\n",
    "\n",
    "# Calculate and store average distances for each document in the DataFrame\n",
    "df['rata_rata_jarak_antar_dokumen_dalam_satu_kluster'] = df.index.map(lambda x: average_distance_within_cluster(x, cluster_indices))\n",
    "\n",
    "# Optional: Display the DataFrame with the new column\n",
    "print(df[['judul_stemmed', 'cluster_label', 'rata_rata_jarak_antar_dokumen_dalam_satu_kluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_distance_to_other_clusters\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate and store average distances to other clusters for each document in the DataFrame\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrata_rata_jarak_antar_dokumen_dengan_kluster_lain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: average_distance_to_other_clusters(x, cluster_indices))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Optional: Display the DataFrame with the new column\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudul_stemmed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrata_rata_jarak_antar_dokumen_dalam_satu_kluster\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrata_rata_jarak_antar_dokumen_dengan_kluster_lain\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Function to calculate average distance from a document to all documents in other clusters\n",
    "def average_distance_to_other_clusters(doc_index, cluster_indices):\n",
    "    cluster_num = df.loc[doc_index, 'cluster_label']\n",
    "    cluster_docs = cluster_indices[cluster_num]\n",
    "\n",
    "    # Calculate pairwise distances between the given document and all documents in other clusters\n",
    "    distances = []\n",
    "    for other_cluster_num, other_cluster_docs in enumerate(cluster_indices):\n",
    "        if other_cluster_num != cluster_num:\n",
    "            distances.extend(pairwise_distances(tfidf_matrix[doc_index], tfidf_matrix[other_cluster_docs], metric='cosine')[0])\n",
    "\n",
    "    # Calculate average distance to other clusters\n",
    "    average_distance_to_other_clusters = sum(distances) / len(distances) if len(distances) > 0 else 0\n",
    "\n",
    "    return average_distance_to_other_clusters\n",
    "\n",
    "# Calculate and store average distances to other clusters for each document in the DataFrame\n",
    "df['rata_rata_jarak_antar_dokumen_dengan_kluster_lain'] = df.index.map(lambda x: average_distance_to_other_clusters(x, cluster_indices))\n",
    "\n",
    "# Optional: Display the DataFrame with the new column\n",
    "print(df[['judul_stemmed', 'cluster_label', 'rata_rata_jarak_antar_dokumen_dalam_satu_kluster', 'rata_rata_jarak_antar_dokumen_dengan_kluster_lain']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
